import argparse
import base64
import json
import os

import requests

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode('utf-8')

def get_gpt_score(image_path=None,answer_gt=None,answer=None):
    # Path to your image
    # print(image_path)
    # OpenAI API Key
    api_key = "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"

    # Getting the base64 string
    # base64_image = encode_image(image_path)
    # base64_image = encode_image(r"img.png")

    prompt='''
        You have been given two detailed descriptions of a surgical scene, the first one is the ground 
        truth (enclosed within the <ground_truth></ground_truth> tags) and the second one is the detailed description generated by a large language model (enclosed within the <generation></generation> tags). 
        I want you to be a surgeon who specializes in operating surgical robots. 
        Please score the accuracy of the detailed description generated by the large language model based on the ground truth in terms of accuracy and quality. 
        The score should be an integer ranging from 0 to 100. Please only output the score.
    '''

    text_prompt=f"<ground_truth>{answer_gt}</ground_truth> \n <generation>{answer}</generation> \n"+prompt
    # print(image_path,text_prompt)
    headers = {
        'Accept': 'application/json',
        'Authorization': 'your_api_key',
        'User-Agent': 'Apifox/1.0.0 (https://apifox.com)',
        'Content-Type': 'application/json'
    }

    payload = {
        "model": "your_llm",
        "messages": [
            {
                "role": "user",
                "content": [
                    {
                        "type": "text",
                        "text": text_prompt
                    },
                ]
            }
        ],
        "max_tokens": 1024,
        "temperature": 0,
    }
    # print(text_prompt)
    response = requests.post("https://api.zhiyungpt.com/v1/chat/completions", headers=headers, json=payload)
    return response

def is_valid_score(s):
    if s.isdigit():  
        num = int(s)
        return 0 <= num <= 100
    return False


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Calculate metrics for model results.')
    parser.add_argument('--result_folder', type=str, required=True, help='Path to the model test result folder')
    args = parser.parse_args()
    model_test_result_folder = args.result_folder
    print("Evaluating model results in folder:", model_test_result_folder)
    # model_test_result_folder="/mnt/data1/bailong/wgk/endochat_revise/test/result/Qwen2.5-VL-7B-Instruct-endochat"
    test_result_path=f"{model_test_result_folder}/test_result.json"
    score_result_path=f"{model_test_result_folder}/detailed_description_score_test.json"
    
    if os.path.exists(score_result_path):
        with open(score_result_path, 'r', encoding='utf-8') as file:
            test_result = json.load(file)
    else:
        with open(test_result_path, 'r', encoding='utf-8') as file:
            test_result = json.load(file)
    
    #detailed description 
    detailed_result=[item for item in test_result if item['main_tag']=="detailed_description"]
    print(len(detailed_result))
        
    scored_ids = [item['id'] for item in detailed_result if 'score' in item]
    print(len(scored_ids))
    print("start evaluating:")
    sum=0
    for i, item in enumerate(detailed_result):
        #
        if "score" not in item:   
            num=1 
            item['score']=None
            score=""
            while num<=3:
                response = get_gpt_score(item['image'], item['answer_gt'], item['answer']) #get response
                print(response.json())
                # print(response.json()["choices"][0]["message"]["content"])
                if "choices" in response.json():
                    score = response.json()["choices"][0]["message"]["content"]
                num+=1
        
                if is_valid_score(score):
                    item['score'] = int(score)
                    print(score)
                    break
                else:
                    # print(score)
                    # item['score'] = 0
                    # break
                    print(f"Invalid score: {score} for item {item['id']}, retrying...")
                
            if i % 10 == 0:
                print(
                    f"-------------------------------------------processing:{i}/{len(detailed_result)}--------------------------------------------")
                with open(score_result_path, "w") as f:
                    json.dump(detailed_result, f, indent=4)

    with open(score_result_path, "w") as f:
        json.dump(detailed_result, f, indent=4)
